{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config Details\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KlDh5wCeiOYs"
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "BASE_MODEL_PATH = \"bert-base-uncased\"\n",
    "MODEL_PATH =\"../pkl_model/train_model_13_07.bin\"\n",
    "TRAINING_FILE = \"../dataset/2000_BIO_taggingdata_ALL_ROW_WISE.csv\"\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rl-ukxNl79pA"
   },
   "source": [
    "## Entity Dataset, Entity Model & Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gwb7zkgxe7Ly"
   },
   "outputs": [],
   "source": [
    "class EntityDataset:\n",
    "    def __init__(self, texts, tags):\n",
    "        # texts: [[\"hi\", \",\", \"my\", \"name\", \"is\", \"abhishek\"], [\"hello\".....]]\n",
    "        # pos/tags: [[1 2 3 4 1 5], [....].....]]\n",
    "        self.texts = texts\n",
    "        #self.pos = pos\n",
    "        self.tags = tags\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        #pos = self.pos[item]\n",
    "        tags = self.tags[item]\n",
    "\n",
    "        ids = []\n",
    "        #target_pos = []\n",
    "        target_tag =[]\n",
    "        \n",
    "        # tokenising for BERT\n",
    "\n",
    "        for i, s in enumerate(text):\n",
    "            inputs = TOKENIZER.encode(\n",
    "                s,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            # abhishek: ab ##hi ##sh ##ek\n",
    "            input_len = len(inputs)\n",
    "            ids.extend(inputs)\n",
    "            #target_pos.extend([pos[i]] * input_len)\n",
    "            target_tag.extend([tags[i]] * input_len)\n",
    "\n",
    "        ids = ids[:MAX_LEN - 2]\n",
    "        # -2 for adding special tokens\n",
    "        #target_pos = target_pos[:config.MAX_LEN - 2]\n",
    "        target_tag = target_tag[:MAX_LEN - 2]\n",
    "\n",
    "        ids = [2] + ids + [3]\n",
    "        #target_pos = [0] + target_pos + [0]\n",
    "        target_tag = [0] + target_tag + [0]\n",
    "\n",
    "        mask = [1] * len(ids)\n",
    "        token_type_ids = [0] * len(ids)\n",
    "\n",
    "        padding_len = MAX_LEN - len(ids)\n",
    "\n",
    "        ids = ids + ([0] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        #target_pos = target_pos + ([0] * padding_len)\n",
    "        target_tag = target_tag + ([0] * padding_len)\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HL0pe_7gh-Re"
   },
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        _, loss = model(**data)\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fAdjBsTbh_Tl"
   },
   "outputs": [],
   "source": [
    "def loss_fn(output, target, mask, num_labels):\n",
    "    lfn = nn.CrossEntropyLoss()\n",
    "    active_loss = mask.view(-1) == 1\n",
    "    active_logits = output.view(-1, num_labels)\n",
    "    active_labels = torch.where(\n",
    "        active_loss,\n",
    "        target.view(-1),\n",
    "        torch.tensor(lfn.ignore_index).type_as(target)\n",
    "    )\n",
    "    loss = lfn(active_logits, active_labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, num_tag):\n",
    "        super(EntityModel, self).__init__()\n",
    "        self.num_tag = num_tag\n",
    "        self.bert = transformers.BertModel.from_pretrained(BASE_MODEL_PATH,return_dict=False)\n",
    "        self.bert_drop_1 = nn.Dropout(0.3)\n",
    "        #self.bert_drop_2 = nn.Dropout(0.3)\n",
    "        self.out_tag = nn.Linear(768, self.num_tag)\n",
    "\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids, target_tag):\n",
    "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        bo_tag = self.bert_drop_1(o1)\n",
    "        #bo_pos = self.bert_drop_2(o1)\n",
    "\n",
    "        tag = self.out_tag(bo_tag)\n",
    "        #pos = self.out_pos(bo_pos)\n",
    "\n",
    "        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n",
    "        #loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n",
    "\n",
    "        #loss = (loss_tag + loss_pos) / 2\n",
    "        loss = loss_tag\n",
    "\n",
    "        return tag, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xOsb2233iLEK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_data(data_path):\n",
    "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "    print('Number of empty values are ', df[\"Word\"].isna().sum())\n",
    "    df[\"Word\"].fillna(\"None\", inplace = True)\n",
    "    enc_pos = preprocessing.LabelEncoder()\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "\n",
    "    #df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n",
    "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
    "    \n",
    "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "    #pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n",
    "    pos = []\n",
    "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
    "    return sentences, pos, tag, enc_pos, enc_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRjpiJmR8FZh"
   },
   "source": [
    "## Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VxDg-i_vs2B",
    "outputId": "9fd93eec-a4a1-4cf0-92e4-806648265c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty values are  0\n"
     ]
    }
   ],
   "source": [
    "sentences, pos, tag, enc_pos, enc_tag = process_data(TRAINING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWTURzDFwgLj",
    "outputId": "aab3b97d-1a81-4db9-fe14-a66e9c8b9966"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta.bin']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = {\n",
    "        \"enc_pos\": enc_pos,\n",
    "        \"enc_tag\": enc_tag\n",
    "    }\n",
    "joblib.dump(meta_data, \"meta.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ln16zDgHwow0"
   },
   "outputs": [],
   "source": [
    "num_tag = len(list(enc_tag.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "31pUpP3MwvWy"
   },
   "outputs": [],
   "source": [
    "(train_sentences, test_sentences, train_tag,test_tag) = model_selection.train_test_split(sentences,  tag, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8yQtgykxAeZ",
    "outputId": "5c1a1faa-f5da-4fac-bbc0-5fb5c5ed9732"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EntityModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert_drop_1): Dropout(p=0.3, inplace=False)\n",
       "  (out_tag): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EntityDataset(\n",
    "        texts=train_sentences, tags=train_tag\n",
    "    )\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4\n",
    "    )\n",
    "\n",
    "valid_dataset = EntityDataset(\n",
    "        texts=test_sentences, tags=test_tag\n",
    "    )\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1\n",
    "    )\n",
    "device = torch.device(\"cuda\")\n",
    "model = EntityModel(num_tag=num_tag)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTURpxw68KAW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UEh1a4YO9WdF"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6SqP7nAxmx-9"
   },
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eBh2OQSAl92n"
   },
   "outputs": [],
   "source": [
    "# for data in tqdm(train_data_loader, total=len(train_data_loader)):\n",
    "#   print('Next Data Loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPtan55B0Lni",
    "outputId": "8733d7ff-6e39-4232-a98c-de6ee2bf7747"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Epoch No :  0 *****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:47<00:00,  1.06it/s]\n",
      "100%|██████████| 51/51 [00:11<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.26925276711583135 Valid Loss = 0.15001491851666393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Epoch No :  1 *****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n",
      "100%|██████████| 51/51 [00:11<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.13164631515741348 Valid Loss = 0.09993404668628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Epoch No :  2 *****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n",
      "100%|██████████| 51/51 [00:11<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.09106713145971299 Valid Loss = 0.08323846775673184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Epoch No :  3 *****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n",
      "100%|██████████| 51/51 [00:11<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.06803649090230465 Valid Loss = 0.07782209817977513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Epoch No :  4 *****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n",
      "100%|██████████| 51/51 [00:11<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.054891033247113225 Valid Loss = 0.07406233837280203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Epoch No :  5 *****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n",
      "100%|██████████| 51/51 [00:11<00:00,  4.61it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.04713058527559042 Valid Loss = 0.07451740897023211\n",
      "\n",
      "**** Epoch No :  6 *****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n",
      "100%|██████████| 51/51 [00:11<00:00,  4.61it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.04132081493735314 Valid Loss = 0.07537634612298479\n",
      "\n",
      "**** Epoch No :  7 *****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n",
      "100%|██████████| 51/51 [00:11<00:00,  4.59it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.03481026720255613 Valid Loss = 0.07600270553181569\n",
      "\n",
      "**** Epoch No :  8 *****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n",
      " 61%|██████    | 31/51 [00:07<00:03,  5.01it/s]"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "num_train_steps = int(len(train_sentences) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"\\n**** Epoch No : \",epoch,\"*****\\n\")\n",
    "    train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "    test_loss = eval_fn(valid_data_loader, model, device)\n",
    "    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
    "    if test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        best_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYjT0RRv8Tj-"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAIbaxjH7pmh",
    "outputId": "94e301df-3057-43b6-e08b-8cbc4e583512"
   },
   "outputs": [],
   "source": [
    "meta_data = joblib.load(\"meta.bin\")\n",
    "enc_tag = meta_data[\"enc_tag\"]\n",
    "\n",
    "#num_pos = len(list(enc_pos.classes_))\n",
    "num_tag = len(list(enc_tag.classes_))\n",
    "sentence = \"\"\"\n",
    "    it professional with an overall 16 years of technocommercial experience in business process consulting implementation and project delivery space for various enterprise business applications and it infrastructure products and services for clients across various industry sectors automotive retail logistics telecom etcexperienced in global service deliveries with distributed teams delivered large implementation roll out programs upgrades data migration and application value managementengagements using different project methodologiesskills summary project scheduling contracts management technical documentation risk management customer relationship management practice development change management team building and mentoring senior project management professional and business process consultant in it industrynnshe has experience in solution design preparing technocommercial proposal product and solution presentation implementation project governance and delivery of enterprise business applicationsnnshe has worked in gulf and india geographies and delivered successful large engagementsnnother skills summary nnproject scheduling nsow management nrisk management npractice development nchange management presales solution design consultant 21 years of experience in business requirement analysis technical solution design commercial architecting projects delivery and leadership and customer advocacy around industry best practices for multiple clients across various industries project delivery management business applications presales solution designing account manager financial services at wipro technologies ltd principal consultant banking bfs at wipro limited presales sap practice at wipro infotech consumer healthcare product development medical device software engineering investment banking front office operations and supply chain\n",
    "    \"\"\"\n",
    "tokenized_sentence = TOKENIZER.encode(sentence)\n",
    "\n",
    "sentence = sentence.split()\n",
    "print(sentence)\n",
    "print(tokenized_sentence)\n",
    "\n",
    "test_dataset = EntityDataset(\n",
    "        texts=[sentence], \n",
    "        tags=[[0] * len(sentence)]\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "pred_model = EntityModel(num_tag=num_tag)\n",
    "pred_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "pred_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  data = test_dataset[0]\n",
    "  for k, v in data.items():\n",
    "    data[k] = v.to(device).unsqueeze(0)\n",
    "  tag, _ = pred_model(**data)\n",
    "\n",
    "print(\n",
    "    enc_tag.inverse_transform(\n",
    "        tag.argmax(2).cpu().numpy().reshape(-1)\n",
    "        )[:len(tokenized_sentence)]\n",
    "    )\n",
    "# print(\n",
    "#     enc_pos.inverse_transform(\n",
    "#         pos.argmax(2).cpu().numpy().reshape(-1)\n",
    "#         )[:len(tokenized_sentence)]\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyBRv7AxQSHA"
   },
   "outputs": [],
   "source": [
    "preds = enc_tag.inverse_transform(\n",
    "        tag.argmax(2).cpu().numpy().reshape(-1)\n",
    "        )[:len(tokenized_sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "VZS6PPMUStoC",
    "outputId": "a0bdbcd3-9a15-4f3f-bfa2-6dd50675be05"
   },
   "outputs": [],
   "source": [
    "TOKENIZER.decode(4003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zS_gbgI7RmuQ",
    "outputId": "6b12e85a-6dc4-4d97-a2c2-a6a2ae6c804a"
   },
   "outputs": [],
   "source": [
    "for elem,cat in zip(tokenized_sentence , preds):\n",
    "  print(TOKENIZER.decode([elem]), '=====>',cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuoxJB45QKSq",
    "outputId": "276d0f2d-ef61-49e6-c103-06e3de7c5e84"
   },
   "outputs": [],
   "source": [
    "tag.argmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5oM8jVDP_n1",
    "outputId": "391ae734-d20d-4c1b-c45e-38abba3f38fb"
   },
   "outputs": [],
   "source": [
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bq1B3p7XQB8C",
    "outputId": "d144e2cc-a207-40ab-a97b-f84695cd90b7"
   },
   "outputs": [],
   "source": [
    "enc_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "wwN0iNDePPNL",
    "outputId": "2e939cf5-4e7c-44af-d59a-ebd3552fa46d"
   },
   "outputs": [],
   "source": [
    "TOKENIZER.decode(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dw-jR1ovpCi"
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
    "    \n",
    "#     meta_data = {\n",
    "#         \"enc_pos\": enc_pos,\n",
    "#         \"enc_tag\": enc_tag\n",
    "#     }\n",
    "\n",
    "#     joblib.dump(meta_data, \"meta.bin\")\n",
    "\n",
    "#     num_pos = len(list(enc_pos.classes_))\n",
    "#     num_tag = len(list(enc_tag.classes_))\n",
    "\n",
    "#     (\n",
    "#         train_sentences,\n",
    "#         test_sentences,\n",
    "#         train_pos,\n",
    "#         test_pos,\n",
    "#         train_tag,\n",
    "#         test_tag\n",
    "#     ) = model_selection.train_test_split(sentences, pos, tag, random_state=42, test_size=0.1)\n",
    "\n",
    "#     train_dataset = dataset.EntityDataset(\n",
    "#         texts=train_sentences, pos=train_pos, tags=train_tag\n",
    "#     )\n",
    "\n",
    "#     train_data_loader = torch.utils.data.DataLoader(\n",
    "#         train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n",
    "#     )\n",
    "\n",
    "#     valid_dataset = dataset.EntityDataset(\n",
    "#         texts=test_sentences, pos=test_pos, tags=test_tag\n",
    "#     )\n",
    "\n",
    "#     valid_data_loader = torch.utils.data.DataLoader(\n",
    "#         valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
    "#     )\n",
    "\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
    "#     model.to(device)\n",
    "\n",
    "#     param_optimizer = list(model.named_parameters())\n",
    "#     no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "#     optimizer_parameters = [\n",
    "#         {\n",
    "#             \"params\": [\n",
    "#                 p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "#             ],\n",
    "#             \"weight_decay\": 0.001,\n",
    "#         },\n",
    "#         {\n",
    "#             \"params\": [\n",
    "#                 p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "#             ],\n",
    "#             \"weight_decay\": 0.0,\n",
    "#         },\n",
    "#     ]\n",
    "\n",
    "#     num_train_steps = int(len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "#     optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "#     scheduler = get_linear_schedule_with_warmup(\n",
    "#         optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "#     )\n",
    "\n",
    "#     best_loss = np.inf\n",
    "#     for epoch in range(config.EPOCHS):\n",
    "#         train_loss = engine.train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "#         test_loss = engine.eval_fn(valid_data_loader, model, device)\n",
    "#         print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
    "#         if test_loss < best_loss:\n",
    "#             torch.save(model.state_dict(), config.MODEL_PATH)\n",
    "#             best_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uFYI8VP3qrq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Train_NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
