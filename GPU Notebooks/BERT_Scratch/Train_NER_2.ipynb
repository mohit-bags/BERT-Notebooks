{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23570,
     "status": "ok",
     "timestamp": 1623923672121,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "A215vhb3e024",
    "outputId": "7723da73-2255-4576-a2f0-da255dec0243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1623923690856,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "s9sKpxn7ilgz",
    "outputId": "5a7d514a-0cb9-4a1f-fd4e-4c46d5322641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased  dataset  model_2000_manually_ROW_WISE  train_model_save\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/drive/My Drive/bert/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-p4NujezMUTX"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFzTo2zCZoEj"
   },
   "outputs": [],
   "source": [
    "# !pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlDh5wCeiOYs"
   },
   "outputs": [],
   "source": [
    "# Config Details\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "import dataset\n",
    "\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 8\n",
    "#BASE_MODEL_PATH = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "BASE_MODEL_PATH = \"bert-base-uncased\"\n",
    "#TOKENIZER_PATH = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "# TOKENIZER_PATH = \"/content/drive/MyDrive/bert-fine-tuned_version_2/tokenizer\"\n",
    "MODEL_PATH =\"/content/drive/MyDrive/bert/train_model_save/model.bin\"\n",
    "TRAINING_FILE = \"/content/drive/My Drive/bert/dataset/BIO_taggingdata_ALL_Spacy_TRAIN.csv\"\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1555,
     "status": "error",
     "timestamp": 1623923841999,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "215QQOJ-ZYxj",
    "outputId": "7a6d5be5-6cab-410e-ff57-9fdc3e419603"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4b83e70061a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/bert/model_2000_manually_ROW_WISE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/simpletransformers/ner/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNERArgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNERModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/simpletransformers/ner/ner_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mAlbertConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mAlbertForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__version__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2709\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2711\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/__init__.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmbart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_mbart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMBartTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmbart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_mbart50\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMBart50Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmt5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMT5Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpegasus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_pegasus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPegasusTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_reformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReformerTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/mt5/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"MT5Tokenizer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mMT5Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"MT5TokenizerFast\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mMT5TokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MT5Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# model = torch.load('/content/drive/MyDrive/bert/model_2000_manually_ROW_WISE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xG0tmIIMf2S"
   },
   "outputs": [],
   "source": [
    "#!cp model.bin /content/drive/MyDrive/model_ner_ft_2.bin\n",
    "#!cp meta.bin /content/drive/MyDrive/meta_ft_2.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OERDUU2OuRbH"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rl-ukxNl79pA"
   },
   "source": [
    "## Entity Dataset, Entity Model & Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwb7zkgxe7Ly"
   },
   "outputs": [],
   "source": [
    "class EntityDataset:\n",
    "    def __init__(self, texts, tags):\n",
    "        # texts: [[\"hi\", \",\", \"my\", \"name\", \"is\", \"abhishek\"], [\"hello\".....]]\n",
    "        # pos/tags: [[1 2 3 4 1 5], [....].....]]\n",
    "        self.texts = texts\n",
    "        #self.pos = pos\n",
    "        self.tags = tags\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        #pos = self.pos[item]\n",
    "        tags = self.tags[item]\n",
    "\n",
    "        ids = []\n",
    "        #target_pos = []\n",
    "        target_tag =[]\n",
    "\n",
    "        for i, s in enumerate(text):\n",
    "            inputs = TOKENIZER.encode(\n",
    "                s,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            # abhishek: ab ##hi ##sh ##ek\n",
    "            input_len = len(inputs)\n",
    "            ids.extend(inputs)\n",
    "            #target_pos.extend([pos[i]] * input_len)\n",
    "            target_tag.extend([tags[i]] * input_len)\n",
    "\n",
    "        ids = ids[:MAX_LEN - 2]\n",
    "        # for CLS/SEP tokens\n",
    "        #target_pos = target_pos[:config.MAX_LEN - 2]\n",
    "        target_tag = target_tag[:MAX_LEN - 2]\n",
    "\n",
    "        ids = [2] + ids + [3]\n",
    "        #target_pos = [0] + target_pos + [0]\n",
    "        target_tag = [0] + target_tag + [0]\n",
    "\n",
    "        mask = [1] * len(ids)\n",
    "        token_type_ids = [0] * len(ids)\n",
    "\n",
    "        padding_len = MAX_LEN - len(ids)\n",
    "\n",
    "        ids = ids + ([0] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        #target_pos = target_pos + ([0] * padding_len)\n",
    "        target_tag = target_tag + ([0] * padding_len)\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HL0pe_7gh-Re"
   },
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        _, loss = model(**data)\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAdjBsTbh_Tl"
   },
   "outputs": [],
   "source": [
    "def loss_fn(output, target, mask, num_labels):\n",
    "    lfn = nn.CrossEntropyLoss()\n",
    "    active_loss = mask.view(-1) == 1\n",
    "    active_logits = output.view(-1, num_labels)\n",
    "    active_labels = torch.where(\n",
    "        active_loss,\n",
    "        target.view(-1),\n",
    "        torch.tensor(lfn.ignore_index).type_as(target)\n",
    "    )\n",
    "    loss = lfn(active_logits, active_labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, num_tag):\n",
    "        super(EntityModel, self).__init__()\n",
    "        self.num_tag = num_tag\n",
    "        self.bert = transformers.BertModel.from_pretrained(BASE_MODEL_PATH,return_dict=False)\n",
    "        self.bert_drop_1 = nn.Dropout(0.3)\n",
    "        #self.bert_drop_2 = nn.Dropout(0.3)\n",
    "        self.out_tag = nn.Linear(768, self.num_tag)\n",
    "\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids, target_tag):\n",
    "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        bo_tag = self.bert_drop_1(o1)\n",
    "        #bo_pos = self.bert_drop_2(o1)\n",
    "\n",
    "        tag = self.out_tag(bo_tag)\n",
    "        #pos = self.out_pos(bo_pos)\n",
    "\n",
    "        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n",
    "        #loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n",
    "\n",
    "        #loss = (loss_tag + loss_pos) / 2\n",
    "        loss = loss_tag\n",
    "\n",
    "        return tag, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOsb2233iLEK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_data(data_path):\n",
    "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "    df=df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "    print('Number of empty values are ', df[\"Word\"].isna().sum())\n",
    "    df[\"Word\"].fillna(\"None\", inplace = True)\n",
    "    df.loc[:, \"Tag\"] = df[\"Tag\"].fillna(method=\"ffill\")\n",
    "\n",
    "    print(\"Check 1\")\n",
    "    enc_pos = preprocessing.LabelEncoder()\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "    print(\"Check 2\")\n",
    "\n",
    "\n",
    "    #df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n",
    "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
    "    print(\"Check 3\")\n",
    "\n",
    "    \n",
    "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "    print(\"Check 4\")\n",
    "\n",
    "    #pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n",
    "    pos = []\n",
    "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
    "    print(\"Check 5\")\n",
    "\n",
    "    return sentences, pos, tag, enc_pos, enc_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRjpiJmR8FZh"
   },
   "source": [
    "## Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3217,
     "status": "ok",
     "timestamp": 1623853464217,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "8VxDg-i_vs2B",
    "outputId": "9aead34c-5b46-41e5-8553-ab635ad20d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty values are  1001\n",
      "Check 1\n",
      "Check 2\n",
      "Check 3\n",
      "Check 4\n",
      "Check 5\n",
      "\n",
      "sentences:\n",
      " [list(['currently', 'employed', 'as', 'a', 'trading', 'agreements', 'specialist', 'at', 'caceis', 'bank', 'luxembourg', 'i', 'am', 'in', 'charge', 'of', 'negotiating', 'standard', 'trading', 'agreements', 'mainly', 'gmsla', 'gmra', 'and', 'isda', 'sharing', 'my', 'knowledge', 'with', 'other', 'legal', 'advisors', 'within', 'the', 'bank', 'and', 'keeping', 'an', 'eye', 'on', 'regulatory', 'improvements', 'likely', 'to', 'affect', 'our', 'current', 'templates', 'and', 'negotiation', 'guidelines', 'eg', 'eu', 'regulations', 'and', 'directives', 'from', 'time', 'to', 'time', 'i', 'get', 'to', 'be', 'involved', 'in', 'the', 'review', 'and', 'the', 'negotiation', 'of', 'tailormade', 'agreements', 'for', 'use', 'in', 'relation', 'to', 'oneshot', 'trades', 'could', 'be', 'trading', 'agreements', 'as', 'well', 'as', 'master', 'confirmation', 'agreements', 'and', 'longform', 'confirmations', 'depending', 'on', 'the', 'operations', 'structured', 'by', 'local', 'trading', 'desksn', 'n', 'i', 'was', 'formerly', 'with', 'amundi', 'alternative', 'investments', 'in', 'paris', 'with', 'a', 'rather', 'similar', 'position', 'yet', 'in', 'relation', 'to', 'hedge', 'funds', 'in', 'a', 'managed', 'accounts', 'platform', 'bermudian', 'segregated', 'accounts', 'company', 'recently', 'hired', 'by', 'clearstream', 'in', 'luxembourg', 'i', 'am', 'facing', 'the', 'challenging', 'expectations', 'of', 'the', 'local', 'collateral', 'management', 'team', 'in', 'terms', 'of', 'client', 'documentation', 'negotiation', 'and', 'new', 'products', 'contractual', 'setup', 'i', 'was', 'formerly', 'employed', 'as', 'a', 'trading', 'agreements', 'specialist', 'at', 'caceis', 'bank', 'luxembourg', 'in', 'charge', 'of', 'negotiating', 'standard', 'trading', 'agreements', 'mainly', 'gmsla', 'gmra', 'and', 'isda', 'also', 'involved', 'in', 'the', 'review', 'and', 'the', 'negotiation', 'of', 'tailormade', 'agreements', 'for', 'use', 'in', 'relation', 'to', 'oneshot', 'trades', 'could', 'be', 'trading', 'agreements', 'as', 'well', 'as', 'master', 'confirmation', 'agreements', 'and', 'longform', 'confirmations', 'structured', 'by', 'local', 'trading', 'desks', 'and', 'partly', 'involved', 'in', 'caceis', 'depositary', 'central', 'administration', 'and', 'transfer', 'agency', 'activities', 'legal', 'issues', 'senior', 'legal', 'advisor', 'at', 'caceis', 'bank', 'luxembourg', 'application', 'development', 'and', 'maintenance', 'senior', 'legal', 'adviser', 'at', 'clearstream', 'investment', 'banking', 'front', 'office', 'retail', 'banking', 'middle', 'office', 'None'])\n",
      " list(['15', 'years', 'combined', 'bpo', 'experience', 'in', 'collections', 'both', 'consumer', 'and', 'b2b', 'programs', 'exposed', 'to', 'complex', 'collection', 'accounts', 'us', 'consumer', 'b2b', 'uk', 'consumer', 'banking', 'australian', 'b2b', 'us', 'transport', 'logistics', 'and', 'skip', 'tracing', 'successfully', 'led', 'supported', 'and', 'managed', 'collection', 'projects', 'from', 'inception', 'migration', 'to', 'bau', 'stage', 'developed', 'strategies', 'improved', 'the', 'upstream', 'process', 'and', 'sustained', 'optimum', 'production', 'numbers', 'that', 'translated', 'to', 'organic', 'growth', 'of', 'programs', 'managed', '15', 'years', 'combined', 'bpo', 'experience', 'in', 'collections', 'and', 'ar', 'management', 'b2c', 'and', 'b2b', 'exposed', 'to', 'complex', 'collection', 'accounts', 'us', 'consumer', 'b2b', 'uk', 'consumer', 'banking', 'australian', 'b2b', 'us', 'transport', 'logistics', 'and', 'skip', 'tracing', 'successfully', 'led', 'supported', 'and', 'managed', 'collection', 'projects', 'from', 'inception', 'migration', 'to', 'bau', 'stage', 'developed', 'strategies', 'improved', 'the', 'upstream', 'process', 'and', 'sustained', 'optimum', 'production', 'numbers', 'that', 'translated', 'to', 'organic', 'growth', 'of', 'programs', 'managed', '11', 'years', 'combined', 'bpo', 'experience', 'in', 'collections', 'both', 'consumer', 'and', 'b2b', 'programs', 'exposed', 'to', 'complex', 'collection', 'programs', 'us', 'consumer', 'b2b', 'uk', 'consumer', 'banking', 'australian', 'b2b', 'logistics', 'and', 'skip', 'tracing', 'successfully', 'led', 'supported', 'and', 'managed', 'pioneer', 'collection', 'projects', 'from', 'inception', 'migrations', 'to', 'bau', 'stage', 'develop', 'strategies', 'improved', 'the', 'upstream', 'process', 'and', 'sustained', 'optimum', 'production', 'numbers', 'that', 'translated', 'to', 'organic', 'growth', 'of', 'programs', 'managed', 'operations', 'manager', 'at', 'sykes', 'manager', 'at', 'probe', 'group', 'None'])\n",
      " list(['kit', 'jang', 'is', 'a', 'final', 'year', 'business', 'finance', 'undergraduate', 'who', 'possesses', 'strong', 'leadership', 'qualities', 'and', 'communication', 'skills', 'along', 'with', 'a', 'caring', 'heart', 'he', 'thrives', 'on', 'challenges', 'that', 'he', 'seeks', 'to', 'always', 'improve', 'himself', 'whilst', 'being', 'humble', 'and', 'retaining', 'his', 'willingness', 'to', 'learn', 'he', 'was', 'the', 'president', 'of', 'smu', 'smiling', 'hearts', 'a', 'studentled', 'community', 'service', 'project', 'aimed', 'at', 'empowering', 'youths', 'he', 'was', 'also', 'part', 'of', 'the', 'organising', 'committee', 'for', 'a', '2013', 'summer', 'camp', 'yolo', 'kit', 'jang', 'has', 'a', 'keen', 'interest', 'in', 'working', 'in', 'deals', 'and', 'asset', 'management', 'corporate', 'finance', 'and', 'advisory', 'sectors', 'in', 'finance', 'he', 'completed', 'a', 'winter', 'internship', 'with', 'pwc', 'deals', 'advisory', 'which', 'provided', 'valuable', 'exposure', 'in', 'the', 'advisory', 'and', 'modelling', 'fields', 'in', 'complementing', 'his', 'overseas', 'exchange', 'program', 'at', 'fudan', 'university', 'school', 'of', 'management', 'in', 'the', 'spring', 'of', '2015', 'he', 'has', 'recently', 'completed', 'his', 'final', 'academic', 'term', 'december', '2015', 'and', 'is', 'waiting', 'for', 'his', 'official', 'graduation', 'in', 'july', '2016', 'in', 'the', 'meantime', 'he', 'is', 'currently', 'enjoying', 'his', 'exposure', 'at', 'primary', 'research', 'startup', 'lynk', 'global', 'until', 'the', 'summer', 'of', '2016', 'our', 'deepest', 'fear', 'is', 'not', 'that', 'we', 'are', 'inadequate', 'our', 'deepest', 'fear', 'is', 'that', 'we', 'are', 'powerful', 'beyond', 'measure', 'it', 'is', 'our', 'light', 'not', 'our', 'darkness', 'that', 'most', 'frightens', 'us', 'coach', 'carter', 'movie', 'this', 'quote', 'is', 'the', 'cornerstone', 'in', 'my', 'life', 'reminding', 'me', 'not', 'to', 'be', 'afraid', 'of', 'being', 'brilliant', 'but', 'always', 'give', 'my', '100', 'in', 'everything', 'that', 'i', 'do', 'and', 'make', 'the', 'best', 'of', 'what', 'i', 'can', 'be', 'through', 'my', 'undergraduate', 'years', 'i', 'have', 'taken', 'on', 'challenges', 'leadership', 'responsibility', 'as', 'well', 'as', 'seeking', 'knowledge', 'across', 'various', 'fields', 'of', 'investment', 'research', 'financial', 'services', 'business', 'development', 'i', 'will', 'be', 'graduating', 'officially', 'from', 'singapore', 'management', 'university', 'in', 'july', '2016', 'bachelor', 'of', 'business', 'management', 'majoring', 'finance', 'eager', 'to', 'meet', 'likeminded', 'individuals', 'in', 'deals', 'management', 'financial', 'advisory', 'feel', 'free', 'to', 'connect', 'via', 'linkedin', 'or', 'drop', 'me', 'an', 'email', 'at', 'kjtan91', 'gmailcom', 'developed', 'technical', 'skills', 'in', 'financial', 'analysis', 'modelling', 'regarding', 'ma', 'valuation', 'dispute', 'resolutions', 'and', 'transactional', 'experience', 'highly', 'motivated', 'selfdriven', 'and', 'adaptable', 'open', 'to', 'seeking', 'new', 'opportunities', 'feel', 'free', 'to', 'contact', 'me', 'at', 'kjtan91', 'gmailcom', 'associate', 'valuations', 'deals', 'team', 'at', 'pwc', 'singapore', 'research', 'and', 'business', 'development', 'analyst', 'at', 'lynk', 'deals', 'advisory', 'intern', 'at', 'pwc', 'singapore', 'None'])\n",
      " ...\n",
      " list(['key', 'accomplishments', 'manage', 'the', 'interior', 'trim', 'team', 'with', '20+', 'employees', 'financial', 'operational', 'and', 'disciplinary', 'responsibility', 'leading', 'develoments', 'teams', 'and', 'supply', 'delieverables', 'according', 'to', 'the', 'development', 'process', 'in', 'cost', 'time', 'and', 'quality', 'a', 'wealth', 'of', 'experience', 'in', 'all', 'areas', 'of', 'automotive', 'interior', 'and', 'development', 'of', 'kinematic', 'as', 'well', 'as', 'static', 'plastic', 'components', 'proficiency', 'in', 'product', 'development', 'with', 'catia', 'v5', 'using', 'a', 'diversity', 'of', 'work', 'benches', 'eg', 'gsd', 'pd', 'fem', 'kin', 'ad', 'optimized', 'product', 'development', 'processes', 'with', 'programming', 'visual', 'basic', 'for', 'applications', 'macros', 'to', 'automate', 'catia', 'v5', 'also', 'in', 'combination', 'with', 'parametric', 'associative', 'constructed', 'cad', 'data', 'key', 'accomplishments', 'manage', 'the', 'interior', 'trim', 'team', 'with', '15', 'employees', 'financial', 'operational', 'and', 'disciplinary', 'responsibility', 'leading', 'develoments', 'teams', 'and', 'supply', 'delieverables', 'according', 'to', 'the', 'development', 'process', 'in', 'cost', 'time', 'and', 'quality', 'a', 'wealth', 'of', 'experience', 'in', 'all', 'areas', 'of', 'automotive', 'interior', 'and', 'development', 'of', 'kinematic', 'as', 'well', 'as', 'static', 'plastic', 'components', 'proficiency', 'in', 'product', 'development', 'with', 'catia', 'v5', 'using', 'a', 'diversity', 'of', 'work', 'benches', 'eg', 'gsd', 'pd', 'fem', 'kin', 'ad', 'optimized', 'product', 'development', 'processes', 'with', 'programming', 'visual', 'basic', 'for', 'applications', 'macros', 'to', 'automate', 'catia', 'v5', 'also', 'in', 'combination', 'with', 'parametric', 'associative', 'constructed', 'cad', 'data', 'key', 'accomplishmentsnnleading', 'the', 'develoment', 'teamnna', 'wealth', 'of', 'experience', 'in', 'all', 'areas', 'of', 'automotive', 'interior', 'and', 'development', 'of', 'kinematic', 'as', 'well', 'as', 'static', 'plastic', 'componentsnnsupervised', 'external', 'suppliers', 'and', 'internal', 'engineers', 'to', 'ensure', 'implementation', 'of', 'global', 'conceptsnnorganized', 'a', 'series', 'of', 'presentations', 'to', 'management', 'illustrating', 'benefits', 'of', 'new', 'conceptsnnproficiency', 'in', 'product', 'development', 'with', 'catia', 'v5', 'using', 'a', 'diversity', 'of', 'work', 'benches', 'eg', 'gsd', 'pd', 'fem', 'kin', 'ad', 'optimized', 'product', 'development', 'processes', 'with', 'programming', 'visual', 'basic', 'for', 'applications', 'macros', 'to', 'automate', 'catia', 'v5', 'also', 'in', 'combination', 'with', 'parametric', 'associative', 'constructed', 'cad', 'data', 'key', 'accomplishments', 'a', 'wealth', 'of', 'experience', 'in', 'all', 'areas', 'of', 'automotive', 'interior', 'and', 'development', 'of', 'kinematic', 'as', 'well', 'as', 'static', 'plastic', 'components', 'supervised', 'external', 'suppliers', 'and', 'internal', 'engineers', 'to', 'ensure', 'implementation', 'of', 'global', 'concepts', 'organized', 'a', 'series', 'of', 'presentations', 'to', 'management', 'illustrating', 'benefits', 'of', 'new', 'concepts', 'proficiency', 'in', 'product', 'development', 'with', 'catia', 'v5', 'using', 'a', 'diversity', 'of', 'work', 'benches', 'eg', 'gsd', 'pd', 'fem', 'kin', 'ad', 'optimized', 'product', 'development', 'processes', 'with', 'programming', 'visual', 'basic', 'for', 'applications', 'macros', 'to', 'automate', 'catia', 'v5', 'also', 'in', 'combination', 'with', 'parametric', 'associative', 'constructed', 'cad', 'data', 'automotive', 'project', 'engineer', 'bei', 'bertrandt', 'ag', 'automate', 'catia', 'v5', 'also', 'in', 'combination', 'with', 'parametric', 'associative', 'constructed', 'cad', 'data', 'vdi', 'ev', 'hamburg', 'None'])\n",
      " list(['microsoft', 'word', 'i', 'am', 'a', 'dual', 'diploma', 'grad', 'with', 'a', 'diploma', 'in', 'graphic', 'design', 'animation', 'and', 'business', 'marketing', 'i', 'am', 'very', 'excited', 'to', 'see', 'what', 'life', 'has', 'to', 'offer', 'now', 'that', 'i', 'am', 'finished', 'schoolnni', 'am', 'a', 'go', 'getter', 'and', 'very', 'versatile', 'in', 'whatever', 'area', 'i', 'am', 'needed', 'in', 'for', 'example', 'in', 'the', 'summers', 'of', '2013', 'and', '2014', 'i', 'helped', 'plan', 'and', 'run', 'ball', 'hockey', 'tournaments', 'in', 'hamilton', 'toronto', 'and', 'barrie', 'and', 'sometimes', 'the', 'organizers', 'had', 'to', 'step', 'in', 'a', 'reff', 'games', 'or', 'pick', 'up', 'garbage', 'during', 'the', 'day', 'if', 'need', 'be', 'i', 'am', 'all', 'for', 'doing', 'whatever', 'is', 'needed', 'to', 'get', 'the', 'job', 'done', 'nnwhen', 'i', 'am', 'not', 'at', 'school', 'working', 'at', 'pro', 'hockey', 'life', 'or', 'playing', 'hockey', 'myself', 'i', 'love', 'to', 'volunteer', 'in', 'areas', 'i', 'am', 'interested', 'in', 'pan', 'am', 'games', '2014', 'esso', 'cup', 'hope', 'volleyball', 'tournament', 'charity', 'ball', 'hockey', 'tournament', 'and', 'the', 'juno', 'awardsnni', 'am', 'eager', 'to', 'learn', 'and', 'to', 'be', 'part', 'of', 'a', 'team', 'if', 'i', 'sound', 'like', 'a', 'possible', 'fit', 'for', 'your', 'company', 'please', 'dont', 'hesitate', 'to', 'contact', 'me', 'and', 'thank', 'you', 'for', 'taking', 'your', 'time', 'to', 'read', 'my', 'profile', 'experienced', 'service', 'supervisor', 'with', 'a', 'demonstrated', 'history', 'of', 'working', 'in', 'the', 'education', 'management', 'industry', 'skilled', 'in', 'market', 'research', 'management', 'adobe', 'photoshop', 'and', 'direct', 'marketing', 'strong', 'consulting', 'professional', 'with', 'a', 'digital', 'marketing', 'online', 'certificate', 'focused', 'in', 'marketing', 'from', 'mcmaster', 'university', 'online', 'solutions', 'specialist', 'at', 'powerschool', 'group', 'llc', 'recent', 'grad', 'looking', 'for', 'new', 'marketing', 'opportunities', 'project', 'manager', 'at', 'powerschool', 'group', 'llc', 'None'])\n",
      " list(['i', 'recently', 'finished', 'my', '4th', 'year', 'as', 'a', 'mechanical', 'engineering', 'undergraduate', 'student', 'at', 'cal', 'poly', 'in', 'san', 'luis', 'obispo', 'i', 'am', 'currently', 'working', 'as', 'a', 'software', 'engineering', 'intern', 'at', 'viakoo', 'inc', 'a', 'company', 'specializing', 'in', 'the', 'software', 'system', 'organization', 'behind', 'physical', 'surveillance', 'infrastructure', 'previous', 'experience', 'in', 'software', 'and', 'mechanical', 'engineering', 'industries', 'has', 'put', 'me', 'in', 'a', 'unique', 'position', 'early', 'in', 'my', 'career', 'i', 'wish', 'to', 'expand', 'my', 'knowledge', 'in', 'cutting', 'edge', 'technology', 'that', 'blends', 'the', 'physical', 'and', 'digital', 'worlds', 'together', 'i', 'will', 'be', 'returning', 'to', 'cal', 'poly', 'for', 'a', '5th', 'year', 'before', 'looking', 'for', 'full', 'time', 'positions', 'in', 'tech', 'i', 'am', 'a', 'fourth', 'year', 'me', 'student', 'at', 'cal', 'poly', 'looking', 'for', 'a', 'mechatronics', 'software', 'internship', 'for', 'the', 'upcoming', 'summer', 'my', 'interests', 'lie', 'in', 'the', 'intersection', 'between', 'hardware', 'and', 'software', 'which', 'is', 'why', 'i', 'am', 'specializing', 'in', 'a', 'mechatronics', 'concentration', 'and', 'pursuing', 'a', 'computer', 'science', 'minor', 'i', 'have', 'previous', 'internship', 'experience', 'in', 'new', 'product', 'development', 'with', 'working', 'in', 'the', 'electrical', 'and', 'electronic', 'manufacturing', 'industry', 'in', 'addition', 'i', 'have', 'a', 'wide', 'range', 'of', 'knowledge', 'in', 'the', 'mechanical', 'engineering', 'realm', 'along', 'with', 'coding', 'skills', 'in', 'systems', 'programming', 'computer', 'architecture', 'python', 'and', 'c', 'student', 'at', 'california', 'polytechnic', 'state', 'university', 'in', 'san', 'luis', 'obispo', 'ca', 'graduating', 'in', 'the', 'spring', 'of', '2019', 'with', 'bs', 'in', 'mechanical', 'engineering', 'and', 'a', 'minor', 'in', 'computer', 'science', 'working', 'as', 'a', 'lead', 'software', 'engineer', 'for', 'ropegun', 'student', 'at', 'california', 'polytechnic', 'state', 'universitysan', 'luis', 'obispo', 'software', 'quality', 'engineering', 'intern', 'at', 'viakoo', 'inc', 'mechanical', 'engineering', 'intern', 'at', 'nordson', 'asymtek', 'None'])]\n",
      "\n",
      " pos:\n",
      " []\n",
      "\n",
      " tag:\n",
      " [list([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      " list([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      " list([0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2])\n",
      " ...\n",
      " list([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      " list([0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      " list([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])]\n",
      "\n",
      " enc_pos:\n",
      " LabelEncoder()\n",
      "\n",
      " enc_tag:\n",
      " LabelEncoder()\n"
     ]
    }
   ],
   "source": [
    "sentences, pos, tag, enc_pos, enc_tag = process_data(TRAINING_FILE)\n",
    "print(\"\\nsentences:\\n\", sentences)\n",
    "print(\"\\n pos:\\n\", pos)\n",
    "print(\"\\n tag:\\n\", tag)\n",
    "print(\"\\n enc_pos:\\n\", enc_pos)\n",
    "print(\"\\n enc_tag:\\n\", enc_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1623853533822,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "IWTURzDFwgLj",
    "outputId": "df63138a-7fa2-4227-ec44-e3a441bcf3ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta.bin']"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = {\n",
    "        \"enc_pos\": enc_pos,\n",
    "        \"enc_tag\": enc_tag\n",
    "    }\n",
    "joblib.dump(meta_data, \"meta.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln16zDgHwow0"
   },
   "outputs": [],
   "source": [
    "num_tag = len(list(enc_tag.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1623853537027,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "Dl8ePtskExoP",
    "outputId": "efe2f93d-c59f-475c-fa65-08fc7fbbd6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(num_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31pUpP3MwvWy"
   },
   "outputs": [],
   "source": [
    "(train_sentences, test_sentences, train_tag,test_tag) = model_selection.train_test_split(sentences,  tag, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2218,
     "status": "ok",
     "timestamp": 1623790231448,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "y8yQtgykxAeZ",
    "outputId": "1c34fdfb-893f-43e3-eb2a-62ef68e63b2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EntityModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert_drop_1): Dropout(p=0.3, inplace=False)\n",
       "  (out_tag): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EntityDataset(\n",
    "        texts=train_sentences, tags=train_tag\n",
    "    )\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=2\n",
    "    )\n",
    "\n",
    "valid_dataset = EntityDataset(\n",
    "        texts=test_sentences, tags=test_tag\n",
    "    )\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1\n",
    "    )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EntityModel(num_tag=num_tag)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTURpxw68KAW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEh1a4YO9WdF"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SqP7nAxmx-9"
   },
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBh2OQSAl92n"
   },
   "outputs": [],
   "source": [
    "# for data in tqdm(train_data_loader, total=len(train_data_loader)):\n",
    "#   print('Next Data Loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1623790584065,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "pPtan55B0Lni",
    "outputId": "e86d3cf0-be77-465c-d064-6fdd7ab8f7f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:27<00:00,  1.11s/it]\n",
      "100%|██████████| 26/26 [00:07<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.2784672385454178 Valid Loss = 0.21914497419045523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████| 26/26 [00:07<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.20560020446777344 Valid Loss = 0.20615282654762268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:28<00:00,  1.13s/it]\n",
      "100%|██████████| 26/26 [00:07<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.18090063333511353 Valid Loss = 0.1459878270442669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:28<00:00,  1.16s/it]\n",
      "100%|██████████| 26/26 [00:07<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.12466846287250519 Valid Loss = 0.11833639586201081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:29<00:00,  1.16s/it]\n",
      "100%|██████████| 26/26 [00:07<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.10317366421222687 Valid Loss = 0.10232346619550999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:29<00:00,  1.17s/it]\n",
      "100%|██████████| 26/26 [00:07<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.0893955871462822 Valid Loss = 0.09557091272794284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:29<00:00,  1.17s/it]\n",
      "100%|██████████| 26/26 [00:07<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.08305739119648933 Valid Loss = 0.08910037233279301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:29<00:00,  1.18s/it]\n",
      "100%|██████████| 26/26 [00:07<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.07843738451600074 Valid Loss = 0.08866441421783887\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "num_train_steps = int(len(train_sentences) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(EPOCHS):\n",
    "  train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "  test_loss = eval_fn(valid_data_loader, model, device)\n",
    "  print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
    "  if test_loss < best_loss:\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "    best_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYjT0RRv8Tj-"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "executionInfo": {
     "elapsed": 379,
     "status": "error",
     "timestamp": 1623853484536,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "UAIbaxjH7pmh",
    "outputId": "530b4744-b5de-4b0f-cc14-2cb5220b9b32"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bbab3688c7c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeta_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menc_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enc_tag\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#num_pos = len(list(enc_pos.classes_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'meta.bin'"
     ]
    }
   ],
   "source": [
    "meta_data = joblib.load(\"meta.bin\")\n",
    "enc_tag = meta_data[\"enc_tag\"]\n",
    "\n",
    "#num_pos = len(list(enc_pos.classes_))\n",
    "num_tag = len(list(enc_tag.classes_))\n",
    "# sentence = \"seasoned backend developer. entrepreneur. open source contributor. scalable, highly-available web development: python (django), ruby (rails, sinatra), node.js, go, react js/native, angular, java. web backend scalability and performance tuning: new relic, ruby-prof, cprofile. queue-based solutions: kue, resque/sidekiq, celery, jms, rabbitmq. mobile: swift, objective-c, restkit/afnetworking, coredata, corelocation, gcd, sentestingkit, android studio, play service, retrofit. deep learning: convolutional neural network. test-driven-development: rspec, cucumber, python unittest, junit, jasmine. continuous integration/delivery: travis, jenkins, capistrano, vagrant, git, subversion, rake, maven, ant, buildout, make/gnu make. cloud/container: aws, azure, docker, docker-compose, dcos, kubernetes. devops: mesos, chef, puppet, mcollective, pxe, ipmi, nagios, zabbix. scripting: bash, python, ruby, perl. open source projects that enjoy 200+ stars on github and 100+k downloads on sourceforge., seasoned backend developer and entrepreneur.\\n\\nspecialties: \\nscalable, highly-available web development: java, ruby (rails, sinatra), python (django), node.js.\\nqueue-based solutions: resque/sidekiq, celery, jms, rabbitmq.\\nweb backend performance tuning.\\nios: coredata, corelocation, gcd, restkit/afnetworking, sentestingkit, swift.\\ntest-driven-development: rspec, cucumber, python unittest, junit.\\ncontinuous integration/delivery: jenkins, chef, capistrano, vagrant, git, subversion, rake, maven, ant, buildout, make/gnu make.\\ninfrastructure-as-a-service: openstack nova, aws ec2.\\nplatform-as-a-service: cloudfoundry, heroku, rightscale, enstratus, scalr, juju.\\ncloud storage: hadoop hdfs, aws s3, openstack swift, mongodb.\\nagile methodologies: scrum, fdd (feature-driven-development).\\ndevops: chef, puppet, mcollective, pxe, ipmi, nagios, zabbix.\\nscripting: bash, python, ruby, perl., programmer. entrepreneur at banian labs, pdh - networking/network engineering, pdh - network planner/provisioning, vp engineering at rhumbix\"\n",
    "# tokenized_sentence = TOKENIZER.encode(sentence)\n",
    "\n",
    "# sentence = sentence.split()\n",
    "# print(sentence)\n",
    "# print(tokenized_sentence)\n",
    "\n",
    "# test_dataset = EntityDataset(\n",
    "#         texts=[sentence], \n",
    "#         tags=[[0] * len(sentence)]\n",
    "#     )\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "# pred_model = EntityModel(num_tag=num_tag)\n",
    "# pred_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "# pred_model.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   data = test_dataset[0]\n",
    "#   for k, v in data.items():\n",
    "#     data[k] = v.to(device).unsqueeze(0)\n",
    "#   tag, _ = pred_model(**data)\n",
    "\n",
    "# print(\n",
    "#     enc_tag.inverse_transform(\n",
    "#         tag.argmax(2).cpu().numpy().reshape(-1)\n",
    "#         )[:len(tokenized_sentence)]\n",
    "#     )\n",
    "# # print(\n",
    "# #     enc_pos.inverse_transform(\n",
    "# #         pos.argmax(2).cpu().numpy().reshape(-1)\n",
    "# #         )[:len(tokenized_sentence)]\n",
    "# #       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6P1VHARLOz-"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# # \n",
    "# train_dataset = EntityDataset(\n",
    "#         texts=train_sentences, tags=train_tag\n",
    "#     )\n",
    "\n",
    "# train_data_loader = torch.utils.data.DataLoader(\n",
    "#         train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=2\n",
    "#     )\n",
    "\n",
    "# valid_dataset = EntityDataset(\n",
    "#         texts=test_sentences, tags=test_tag\n",
    "#     )\n",
    "\n",
    "# valid_data_loader = torch.utils.data.DataLoader(\n",
    "#         valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1\n",
    "#     )\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = EntityModel(num_tag=num_tag)\n",
    "# model.to(device)\n",
    "# # \n",
    "\n",
    "for i in df[\"summaries\"]:\n",
    "  sentence = i\n",
    "  tokenized_sentence = TOKENIZER.encode(sentence)\n",
    "  sentence = sentence.split()\n",
    "  print(\"******** sentence ********\", sentence)\n",
    "  print(\"******** tokenised sentence ********\", tokenized_sentence)\n",
    "  test_dataset = EntityDataset(\n",
    "        texts=[sentence], \n",
    "        tags=[[0] * len(sentence)]\n",
    "    )\n",
    "  pred_model = EntityModel(num_tag=num_tag)\n",
    "  pred_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "  pred_model.to(device)\n",
    "  with torch.no_grad():\n",
    "    data = test_dataset[0]\n",
    "    for k, v in data.items():\n",
    "      data[k] = v.to(device).unsqueeze(0)\n",
    "      tag, _ = pred_model(**data)\n",
    "\n",
    "    preds = enc_tag.inverse_transform(\n",
    "        tag.argmax(2).cpu().numpy().reshape(-1)\n",
    "        )[:len(tokenized_sentence)]\n",
    "\n",
    "    for elem,cat in zip(tokenized_sentence , preds):\n",
    "      print(TOKENIZER.decode([elem]), '=====>',cat)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# sentence = \"seasoned backend developer. entrepreneur. open source contributor. scalable, highly-available web development: python (django), ruby (rails, sinatra), node.js, go, react js/native, angular, java. web backend scalability and performance tuning: new relic, ruby-prof, cprofile. queue-based solutions: kue, resque/sidekiq, celery, jms, rabbitmq. mobile: swift, objective-c, restkit/afnetworking, coredata, corelocation, gcd, sentestingkit, android studio, play service, retrofit. deep learning: convolutional neural network. test-driven-development: rspec, cucumber, python unittest, junit, jasmine. continuous integration/delivery: travis, jenkins, capistrano, vagrant, git, subversion, rake, maven, ant, buildout, make/gnu make. cloud/container: aws, azure, docker, docker-compose, dcos, kubernetes. devops: mesos, chef, puppet, mcollective, pxe, ipmi, nagios, zabbix. scripting: bash, python, ruby, perl. open source projects that enjoy 200+ stars on github and 100+k downloads on sourceforge., seasoned backend developer and entrepreneur.\\n\\nspecialties: \\nscalable, highly-available web development: java, ruby (rails, sinatra), python (django), node.js.\\nqueue-based solutions: resque/sidekiq, celery, jms, rabbitmq.\\nweb backend performance tuning.\\nios: coredata, corelocation, gcd, restkit/afnetworking, sentestingkit, swift.\\ntest-driven-development: rspec, cucumber, python unittest, junit.\\ncontinuous integration/delivery: jenkins, chef, capistrano, vagrant, git, subversion, rake, maven, ant, buildout, make/gnu make.\\ninfrastructure-as-a-service: openstack nova, aws ec2.\\nplatform-as-a-service: cloudfoundry, heroku, rightscale, enstratus, scalr, juju.\\ncloud storage: hadoop hdfs, aws s3, openstack swift, mongodb.\\nagile methodologies: scrum, fdd (feature-driven-development).\\ndevops: chef, puppet, mcollective, pxe, ipmi, nagios, zabbix.\\nscripting: bash, python, ruby, perl., programmer. entrepreneur at banian labs, pdh - networking/network engineering, pdh - network planner/provisioning, vp engineering at rhumbix\"\n",
    "# tokenized_sentence = TOKENIZER.encode(sentence)\n",
    "\n",
    "# sentence = sentence.split()\n",
    "# print(sentence)\n",
    "# print(tokenized_sentence)\n",
    "\n",
    "# test_dataset = EntityDataset(\n",
    "#         texts=[sentence], \n",
    "#         tags=[[0] * len(sentence)]\n",
    "#     )\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "# pred_model = EntityModel(num_tag=num_tag)\n",
    "# pred_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "# pred_model.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   data = test_dataset[0]\n",
    "#   for k, v in data.items():\n",
    "#     data[k] = v.to(device).unsqueeze(0)\n",
    "#   tag, _ = pred_model(**data)\n",
    "\n",
    "# print(\n",
    "#     enc_tag.inverse_transform(\n",
    "#         tag.argmax(2).cpu().numpy().reshape(-1)\n",
    "#         )[:len(tokenized_sentence)]\n",
    "#     )\n",
    "# # print(\n",
    "# #     enc_pos.inverse_transform(\n",
    "# #         pos.argmax(2).cpu().numpy().reshape(-1)\n",
    "# #         )[:len(tokenized_sentence)]\n",
    "# #       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyBRv7AxQSHA"
   },
   "outputs": [],
   "source": [
    "preds = enc_tag.inverse_transform(\n",
    "        tag.argmax(2).cpu().numpy().reshape(-1)\n",
    "        )[:len(tokenized_sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1623790891871,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "VZS6PPMUStoC",
    "outputId": "76e9cd02-3514-4824-d265-6446765852d7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'# # ᄌ'"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.decode(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1623790892858,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "zS_gbgI7RmuQ",
    "outputId": "71cf2dcb-fd61-49bd-b2ec-1c11c92b8856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] =====> B-ORG\n",
      "seasoned =====> O\n",
      "back =====> O\n",
      "##end =====> O\n",
      "developer =====> O\n",
      ". =====> O\n",
      "entrepreneur =====> O\n",
      ". =====> O\n",
      "open =====> O\n",
      "source =====> O\n",
      "contributor =====> O\n",
      ". =====> O\n",
      "scala =====> O\n",
      "##ble =====> O\n",
      ", =====> O\n",
      "highly =====> O\n",
      "- =====> O\n",
      "available =====> O\n",
      "web =====> O\n",
      "development =====> O\n",
      ": =====> O\n",
      "python =====> B-ORG\n",
      "( =====> O\n",
      "dj =====> O\n",
      "##ango =====> O\n",
      ") =====> O\n",
      ", =====> O\n",
      "ruby =====> O\n",
      "( =====> O\n",
      "rails =====> O\n",
      ", =====> O\n",
      "sinatra =====> O\n",
      ") =====> O\n",
      ", =====> O\n",
      "node =====> O\n",
      ". =====> O\n",
      "j =====> O\n",
      "##s =====> O\n",
      ", =====> O\n",
      "go =====> O\n",
      ", =====> O\n",
      "react =====> O\n",
      "j =====> O\n",
      "##s =====> O\n",
      "/ =====> O\n",
      "native =====> O\n",
      ", =====> O\n",
      "angular =====> O\n",
      ", =====> O\n",
      "java =====> B-ORG\n",
      ". =====> O\n",
      "web =====> O\n",
      "back =====> O\n",
      "##end =====> O\n",
      "scala =====> O\n",
      "##bility =====> O\n",
      "and =====> O\n",
      "performance =====> O\n",
      "tuning =====> O\n",
      ": =====> O\n",
      "new =====> O\n",
      "relic =====> O\n",
      ", =====> O\n",
      "ruby =====> O\n",
      "- =====> O\n",
      "prof =====> O\n",
      ", =====> O\n",
      "cp =====> O\n",
      "##ro =====> O\n",
      "##fi =====> O\n",
      "##le =====> O\n",
      ". =====> O\n",
      "queue =====> O\n",
      "- =====> O\n",
      "based =====> O\n",
      "solutions =====> O\n",
      ": =====> O\n",
      "ku =====> O\n",
      "##e =====> O\n",
      ", =====> O\n",
      "res =====> O\n",
      "##que =====> O\n",
      "/ =====> O\n",
      "side =====> O\n",
      "##ki =====> O\n",
      "##q =====> O\n",
      ", =====> O\n",
      "ce =====> O\n",
      "##ler =====> B-ORG\n",
      "##y =====> O\n",
      ", =====> O\n",
      "j =====> O\n",
      "##ms =====> O\n",
      ", =====> O\n",
      "rabbit =====> O\n",
      "##m =====> O\n",
      "##q =====> B-ORG\n",
      ". =====> O\n",
      "mobile =====> O\n",
      ": =====> O\n",
      "swift =====> O\n",
      ", =====> O\n",
      "objective =====> O\n",
      "- =====> O\n",
      "c =====> O\n",
      ", =====> O\n",
      "rest =====> O\n",
      "##kit =====> O\n",
      "/ =====> O\n",
      "af =====> O\n",
      "##net =====> O\n",
      "##working =====> O\n",
      ", =====> O\n",
      "core =====> O\n",
      "##da =====> O\n",
      "##ta =====> O\n",
      ", =====> O\n",
      "core =====> O\n",
      "##lo =====> O\n",
      "##cation =====> O\n",
      ", =====> O\n",
      "g =====> O\n",
      "##cd =====> O\n",
      ", =====> O\n",
      "sent =====> O\n",
      "##est =====> O\n",
      "##ing =====> O\n",
      "##kit =====> B-ORG\n"
     ]
    }
   ],
   "source": [
    "for elem,cat in zip(tokenized_sentence , preds):\n",
    "  print(TOKENIZER.decode([elem]), '=====>',cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1623790895886,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "CuoxJB45QKSq",
    "outputId": "fd39af4b-f621-4584-b8e9-e2920688aed4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n",
       "         0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag.argmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1623790897816,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "h5oM8jVDP_n1",
    "outputId": "c42a5cc3-ebca-40ee-9355-2477693261c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 28223,\n",
       " 2067,\n",
       " 10497,\n",
       " 9722,\n",
       " 1012,\n",
       " 10670,\n",
       " 1012,\n",
       " 2330,\n",
       " 3120,\n",
       " 12130,\n",
       " 1012,\n",
       " 26743,\n",
       " 3468,\n",
       " 1010,\n",
       " 3811,\n",
       " 1011,\n",
       " 2800,\n",
       " 4773,\n",
       " 2458,\n",
       " 1024,\n",
       " 18750,\n",
       " 1006,\n",
       " 6520,\n",
       " 23422,\n",
       " 1007,\n",
       " 1010,\n",
       " 10090,\n",
       " 1006,\n",
       " 15168,\n",
       " 1010,\n",
       " 19643,\n",
       " 1007,\n",
       " 1010,\n",
       " 13045,\n",
       " 1012,\n",
       " 1046,\n",
       " 2015,\n",
       " 1010,\n",
       " 2175,\n",
       " 1010,\n",
       " 10509,\n",
       " 1046,\n",
       " 2015,\n",
       " 1013,\n",
       " 3128,\n",
       " 1010,\n",
       " 16108,\n",
       " 1010,\n",
       " 9262,\n",
       " 1012,\n",
       " 4773,\n",
       " 2067,\n",
       " 10497,\n",
       " 26743,\n",
       " 8553,\n",
       " 1998,\n",
       " 2836,\n",
       " 17372,\n",
       " 1024,\n",
       " 2047,\n",
       " 24933,\n",
       " 1010,\n",
       " 10090,\n",
       " 1011,\n",
       " 11268,\n",
       " 1010,\n",
       " 18133,\n",
       " 3217,\n",
       " 8873,\n",
       " 2571,\n",
       " 1012,\n",
       " 24240,\n",
       " 1011,\n",
       " 2241,\n",
       " 7300,\n",
       " 1024,\n",
       " 13970,\n",
       " 2063,\n",
       " 1010,\n",
       " 24501,\n",
       " 4226,\n",
       " 1013,\n",
       " 2217,\n",
       " 3211,\n",
       " 4160,\n",
       " 1010,\n",
       " 8292,\n",
       " 3917,\n",
       " 2100,\n",
       " 1010,\n",
       " 1046,\n",
       " 5244,\n",
       " 1010,\n",
       " 10442,\n",
       " 2213,\n",
       " 4160,\n",
       " 1012,\n",
       " 4684,\n",
       " 1024,\n",
       " 9170,\n",
       " 1010,\n",
       " 7863,\n",
       " 1011,\n",
       " 1039,\n",
       " 1010,\n",
       " 2717,\n",
       " 23615,\n",
       " 1013,\n",
       " 21358,\n",
       " 7159,\n",
       " 21398,\n",
       " 1010,\n",
       " 4563,\n",
       " 2850,\n",
       " 2696,\n",
       " 1010,\n",
       " 4563,\n",
       " 4135,\n",
       " 10719,\n",
       " 1010,\n",
       " 1043,\n",
       " 19797,\n",
       " 1010,\n",
       " 2741,\n",
       " 4355,\n",
       " 2075,\n",
       " 23615,\n",
       " 1010,\n",
       " 11924,\n",
       " 2996,\n",
       " 1010,\n",
       " 2377,\n",
       " 2326,\n",
       " 1010,\n",
       " 22307,\n",
       " 8873,\n",
       " 2102,\n",
       " 1012,\n",
       " 2784,\n",
       " 4083,\n",
       " 1024,\n",
       " 9530,\n",
       " 6767,\n",
       " 7630,\n",
       " 3508,\n",
       " 2389,\n",
       " 15756,\n",
       " 2897,\n",
       " 1012,\n",
       " 3231,\n",
       " 1011,\n",
       " 5533,\n",
       " 1011,\n",
       " 2458,\n",
       " 1024,\n",
       " 12667,\n",
       " 5051,\n",
       " 2278,\n",
       " 1010,\n",
       " 12731,\n",
       " 24894,\n",
       " 5677,\n",
       " 1010,\n",
       " 18750,\n",
       " 3131,\n",
       " 22199,\n",
       " 1010,\n",
       " 12022,\n",
       " 4183,\n",
       " 1010,\n",
       " 14032,\n",
       " 1012,\n",
       " 7142,\n",
       " 8346,\n",
       " 1013,\n",
       " 6959,\n",
       " 1024,\n",
       " 10001,\n",
       " 1010,\n",
       " 11098,\n",
       " 1010,\n",
       " 6178,\n",
       " 2923,\n",
       " 20770,\n",
       " 1010,\n",
       " 12436,\n",
       " 18980,\n",
       " 1010,\n",
       " 21025,\n",
       " 2102,\n",
       " 1010,\n",
       " 4942,\n",
       " 27774,\n",
       " 1010,\n",
       " 26008,\n",
       " 1010,\n",
       " 5003,\n",
       " 8159,\n",
       " 1010,\n",
       " 14405,\n",
       " 1010,\n",
       " 3857,\n",
       " 5833,\n",
       " 1010,\n",
       " 2191,\n",
       " 1013,\n",
       " 27004,\n",
       " 2191,\n",
       " 1012,\n",
       " 6112,\n",
       " 1013,\n",
       " 11661,\n",
       " 1024,\n",
       " 22091,\n",
       " 2015,\n",
       " 1010,\n",
       " 24296,\n",
       " 1010,\n",
       " 8946,\n",
       " 2121,\n",
       " 1010,\n",
       " 8946,\n",
       " 2121,\n",
       " 1011,\n",
       " 17202,\n",
       " 1010,\n",
       " 5887,\n",
       " 2891,\n",
       " 1010,\n",
       " 13970,\n",
       " 5677,\n",
       " 7159,\n",
       " 2229,\n",
       " 1012,\n",
       " 16475,\n",
       " 11923,\n",
       " 1024,\n",
       " 2033,\n",
       " 17063,\n",
       " 1010,\n",
       " 10026,\n",
       " 1010,\n",
       " 13997,\n",
       " 1010,\n",
       " 11338,\n",
       " 14511,\n",
       " 22471,\n",
       " 3512,\n",
       " 1010,\n",
       " 1052,\n",
       " 2595,\n",
       " 2063,\n",
       " 1010,\n",
       " 12997,\n",
       " 4328,\n",
       " 1010,\n",
       " 6583,\n",
       " 11411,\n",
       " 2015,\n",
       " 1010,\n",
       " 23564,\n",
       " 10322,\n",
       " 7646,\n",
       " 1012,\n",
       " 5896,\n",
       " 2075,\n",
       " 1024,\n",
       " 24234,\n",
       " 1010,\n",
       " 18750,\n",
       " 1010,\n",
       " 10090,\n",
       " 1010,\n",
       " 2566,\n",
       " 2140,\n",
       " 1012,\n",
       " 2330,\n",
       " 3120,\n",
       " 3934,\n",
       " 2008,\n",
       " 5959,\n",
       " 3263,\n",
       " 1009,\n",
       " 3340,\n",
       " 2006,\n",
       " 21025,\n",
       " 2705,\n",
       " 12083,\n",
       " 1998,\n",
       " 2531,\n",
       " 1009,\n",
       " 1047,\n",
       " 22956,\n",
       " 2006,\n",
       " 3120,\n",
       " 29278,\n",
       " 3351,\n",
       " 1012,\n",
       " 1010,\n",
       " 28223,\n",
       " 2067,\n",
       " 10497,\n",
       " 9722,\n",
       " 1998,\n",
       " 10670,\n",
       " 1012,\n",
       " 2569,\n",
       " 7368,\n",
       " 1024,\n",
       " 26743,\n",
       " 3468,\n",
       " 1010,\n",
       " 3811,\n",
       " 1011,\n",
       " 2800,\n",
       " 4773,\n",
       " 2458,\n",
       " 1024,\n",
       " 9262,\n",
       " 1010,\n",
       " 10090,\n",
       " 1006,\n",
       " 15168,\n",
       " 1010,\n",
       " 19643,\n",
       " 1007,\n",
       " 1010,\n",
       " 18750,\n",
       " 1006,\n",
       " 6520,\n",
       " 23422,\n",
       " 1007,\n",
       " 1010,\n",
       " 13045,\n",
       " 1012,\n",
       " 1046,\n",
       " 2015,\n",
       " 1012,\n",
       " 24240,\n",
       " 1011,\n",
       " 2241,\n",
       " 7300,\n",
       " 1024,\n",
       " 24501,\n",
       " 4226,\n",
       " 1013,\n",
       " 2217,\n",
       " 3211,\n",
       " 4160,\n",
       " 1010,\n",
       " 8292,\n",
       " 3917,\n",
       " 2100,\n",
       " 1010,\n",
       " 1046,\n",
       " 5244,\n",
       " 1010,\n",
       " 10442,\n",
       " 2213,\n",
       " 4160,\n",
       " 1012,\n",
       " 4773,\n",
       " 2067,\n",
       " 10497,\n",
       " 2836,\n",
       " 17372,\n",
       " 1012,\n",
       " 16380,\n",
       " 1024,\n",
       " 4563,\n",
       " 2850,\n",
       " 2696,\n",
       " 1010,\n",
       " 4563,\n",
       " 4135,\n",
       " 10719,\n",
       " 1010,\n",
       " 1043,\n",
       " 19797,\n",
       " 1010,\n",
       " 2717,\n",
       " 23615,\n",
       " 1013,\n",
       " 21358,\n",
       " 7159,\n",
       " 21398,\n",
       " 1010,\n",
       " 2741,\n",
       " 4355,\n",
       " 2075,\n",
       " 23615,\n",
       " 1010,\n",
       " 9170,\n",
       " 1012,\n",
       " 3231,\n",
       " 1011,\n",
       " 5533,\n",
       " 1011,\n",
       " 2458,\n",
       " 1024,\n",
       " 12667,\n",
       " 5051,\n",
       " 2278,\n",
       " 1010,\n",
       " 12731,\n",
       " 24894,\n",
       " 5677,\n",
       " 1010,\n",
       " 18750,\n",
       " 3131,\n",
       " 22199,\n",
       " 1010,\n",
       " 12022,\n",
       " 4183,\n",
       " 1012,\n",
       " 7142,\n",
       " 8346,\n",
       " 1013,\n",
       " 6959,\n",
       " 1024,\n",
       " 11098,\n",
       " 1010,\n",
       " 10026,\n",
       " 1010,\n",
       " 6178,\n",
       " 2923,\n",
       " 20770,\n",
       " 1010,\n",
       " 12436,\n",
       " 18980,\n",
       " 1010,\n",
       " 21025,\n",
       " 2102,\n",
       " 1010,\n",
       " 4942,\n",
       " 27774,\n",
       " 1010,\n",
       " 26008,\n",
       " 1010,\n",
       " 5003,\n",
       " 8159,\n",
       " 1010,\n",
       " 14405,\n",
       " 1010,\n",
       " 3857,\n",
       " 5833,\n",
       " 1010,\n",
       " 2191,\n",
       " 1013,\n",
       " 27004,\n",
       " 2191,\n",
       " 1012,\n",
       " 6502,\n",
       " 1011,\n",
       " 2004,\n",
       " 1011,\n",
       " 1037,\n",
       " 1011,\n",
       " 2326,\n",
       " 1024,\n",
       " 7480,\n",
       " 2696,\n",
       " 3600,\n",
       " 6846,\n",
       " 1010,\n",
       " 22091,\n",
       " 2015,\n",
       " 14925,\n",
       " 2475,\n",
       " 1012,\n",
       " 4132,\n",
       " 1011,\n",
       " 2004,\n",
       " 1011,\n",
       " 1037,\n",
       " 1011,\n",
       " 2326,\n",
       " 1024,\n",
       " 6112,\n",
       " 14876,\n",
       " 8630,\n",
       " 2854,\n",
       " 1010,\n",
       " 5394,\n",
       " 5283,\n",
       " 1010,\n",
       " 2916,\n",
       " 9289,\n",
       " 2063,\n",
       " 1010,\n",
       " 4372,\n",
       " 20528,\n",
       " 5809,\n",
       " 1010,\n",
       " 8040,\n",
       " 2389,\n",
       " 2099,\n",
       " 1010,\n",
       " 18414,\n",
       " 9103,\n",
       " 1012,\n",
       " 6112,\n",
       " 5527,\n",
       " 1024,\n",
       " 2018,\n",
       " 18589,\n",
       " 10751,\n",
       " 10343,\n",
       " 1010,\n",
       " 22091,\n",
       " 2015,\n",
       " 1055,\n",
       " 2509,\n",
       " 1010,\n",
       " 7480,\n",
       " 2696,\n",
       " 3600,\n",
       " 9170,\n",
       " 1010,\n",
       " 12256,\n",
       " 3995,\n",
       " 18939,\n",
       " 1012,\n",
       " 29003,\n",
       " 4118,\n",
       " 20792,\n",
       " 1024,\n",
       " 8040,\n",
       " 6824,\n",
       " 1010,\n",
       " 1042,\n",
       " 14141,\n",
       " 1006,\n",
       " 3444,\n",
       " 1011,\n",
       " 5533,\n",
       " 1011,\n",
       " 2458,\n",
       " 1007,\n",
       " 1012,\n",
       " 16475,\n",
       " 11923,\n",
       " 1024,\n",
       " 10026,\n",
       " 1010,\n",
       " 13997,\n",
       " 1010,\n",
       " 11338,\n",
       " 14511,\n",
       " 22471,\n",
       " 3512,\n",
       " 1010,\n",
       " 1052,\n",
       " 2595,\n",
       " 2063,\n",
       " 1010,\n",
       " 12997,\n",
       " 4328,\n",
       " 1010,\n",
       " 6583,\n",
       " 11411,\n",
       " 2015,\n",
       " 1010,\n",
       " 23564,\n",
       " 10322,\n",
       " 7646,\n",
       " 1012,\n",
       " 5896,\n",
       " 2075,\n",
       " 1024,\n",
       " 24234,\n",
       " 1010,\n",
       " 18750,\n",
       " 1010,\n",
       " 10090,\n",
       " 1010,\n",
       " 2566,\n",
       " 2140,\n",
       " 1012,\n",
       " 1010,\n",
       " 20273,\n",
       " 1012,\n",
       " 10670,\n",
       " 2012,\n",
       " 7221,\n",
       " 2937,\n",
       " 13625,\n",
       " 1010,\n",
       " 22851,\n",
       " 2232,\n",
       " 1011,\n",
       " 14048,\n",
       " 1013,\n",
       " 2897,\n",
       " 3330,\n",
       " 1010,\n",
       " 22851,\n",
       " 2232,\n",
       " 1011,\n",
       " 2897,\n",
       " 24555,\n",
       " 1013,\n",
       " 9347,\n",
       " 2075,\n",
       " 1010,\n",
       " 21210,\n",
       " 3330,\n",
       " 2012,\n",
       " 1054,\n",
       " 28600,\n",
       " 5638,\n",
       " 2595,\n",
       " 102]"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1623790898307,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "bq1B3p7XQB8C",
    "outputId": "b49d51e2-d98c-4efe-8875-78dd8fee9875"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1623790898308,
     "user": {
      "displayName": "Shubham Kumar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhSasqlkSGouVaJ4_6HwZYs3ccarzD6XSfX3qG7=s64",
      "userId": "11001256407696526704"
     },
     "user_tz": -330
    },
    "id": "wwN0iNDePPNL",
    "outputId": "a9e40a96-1fcd-42ab-ee6b-e08c43bdece4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] seasoned backend developer. entrepreneur. open source contributor. scalable, highly - available web development : python ( django ), ruby ( rails, sinatra ), node. js, go, react js / native, angular, java. web backend scalability and performance tuning : new relic, ruby - prof, cprofile. queue - based solutions : kue, resque / sidekiq, celery, jms, rabbitmq. mobile : swift, objective - c, restkit / afnetworking, coredata, corelocation, gcd, sentestingkit, android studio, play service, retrofit. deep learning : convolutional neural network. test - driven - development : rspec, cucumber, python unittest, junit, jasmine. continuous integration / delivery : travis, jenkins, capistrano, vagrant, git, subversion, rake, maven, ant, buildout, make / gnu make. cloud / container : aws, azure, docker, docker - compose, dcos, kubernetes. devops : mesos, chef, puppet, mcollective, pxe, ipmi, nagios, zabbix. scripting : bash, python, ruby, perl. open source projects that enjoy 200 + stars on github and 100 + k downloads on sourceforge., seasoned backend developer and entrepreneur. specialties : scalable, highly - available web development : java, ruby ( rails, sinatra ), python ( django ), node. js. queue - based solutions : resque / sidekiq, celery, jms, rabbitmq. web backend performance tuning. ios : coredata, corelocation, gcd, restkit / afnetworking, sentestingkit, swift. test - driven - development : rspec, cucumber, python unittest, junit. continuous integration / delivery : jenkins, chef, capistrano, vagrant, git, subversion, rake, maven, ant, buildout, make / gnu make. infrastructure - as - a - service : openstack nova, aws ec2. platform - as - a - service : cloudfoundry, heroku, rightscale, enstratus, scalr, juju. cloud storage : hadoop hdfs, aws s3, openstack swift, mongodb. agile methodologies : scrum, fdd ( feature - driven - development ). devops : chef, puppet, mcollective, pxe, ipmi, nagios, zabbix. scripting : bash, python, ruby, perl., programmer. entrepreneur at banian labs, pdh - networking / network engineering, pdh - network planner / provisioning, vp engineering at rhumbix [SEP]'"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.decode(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dw-jR1ovpCi"
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
    "    \n",
    "#     meta_data = {\n",
    "#         \"enc_pos\": enc_pos,\n",
    "#         \"enc_tag\": enc_tag\n",
    "#     }\n",
    "\n",
    "#     joblib.dump(meta_data, \"meta.bin\")\n",
    "\n",
    "#     num_pos = len(list(enc_pos.classes_))\n",
    "#     num_tag = len(list(enc_tag.classes_))\n",
    "\n",
    "#     (\n",
    "#         train_sentences,\n",
    "#         test_sentences,\n",
    "#         train_pos,\n",
    "#         test_pos,\n",
    "#         train_tag,\n",
    "#         test_tag\n",
    "#     ) = model_selection.train_test_split(sentences, pos, tag, random_state=42, test_size=0.1)\n",
    "\n",
    "#     train_dataset = dataset.EntityDataset(\n",
    "#         texts=train_sentences, pos=train_pos, tags=train_tag\n",
    "#     )\n",
    "\n",
    "#     train_data_loader = torch.utils.data.DataLoader(\n",
    "#         train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n",
    "#     )\n",
    "\n",
    "#     valid_dataset = dataset.EntityDataset(\n",
    "#         texts=test_sentences, pos=test_pos, tags=test_tag\n",
    "#     )\n",
    "\n",
    "#     valid_data_loader = torch.utils.data.DataLoader(\n",
    "#         valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
    "#     )\n",
    "\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
    "#     model.to(device)\n",
    "\n",
    "#     param_optimizer = list(model.named_parameters())\n",
    "#     no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "#     optimizer_parameters = [\n",
    "#         {\n",
    "#             \"params\": [\n",
    "#                 p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "#             ],\n",
    "#             \"weight_decay\": 0.001,\n",
    "#         },\n",
    "#         {\n",
    "#             \"params\": [\n",
    "#                 p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "#             ],\n",
    "#             \"weight_decay\": 0.0,\n",
    "#         },\n",
    "#     ]\n",
    "\n",
    "#     num_train_steps = int(len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "#     optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "#     scheduler = get_linear_schedule_with_warmup(\n",
    "#         optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "#     )\n",
    "\n",
    "#     best_loss = np.inf\n",
    "#     for epoch in range(config.EPOCHS):\n",
    "#         train_loss = engine.train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "#         test_loss = engine.eval_fn(valid_data_loader, model, device)\n",
    "#         print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
    "#         if test_loss < best_loss:\n",
    "#             torch.save(model.state_dict(), config.MODEL_PATH)\n",
    "#             best_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uFYI8VP3qrq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Train_NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
