{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv(\"tech_sort1k.csv\")\n",
    "# data.head(15)\n",
    "\n",
    "data=data.drop(columns=[\"id\",\"Note\"])\n",
    "\n",
    "data.head()\n",
    "\n",
    "#replacing null vals with empty list\n",
    "data['exact_matched_patt_contextual'] = [ [] if x is np.NaN else x for x in data['exact_matched_patt_contextual'] ]\n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    return text\n",
    "\n",
    "\n",
    "# for i, l in enumerate(data[\"exact_matched_patt_contextual\"]):\n",
    "#  print(\"list\",i,\"is\",type(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing null vals with empty list\n",
    "data['exact_matched_patt_contextual'] = [ [] if x is np.NaN else x for x in data['exact_matched_patt_contextual'] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohitbagaria/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, l in enumerate(data[\"exact_matched_patt_contextual\"]):\n",
    "#  print(\"list\",i,\"is\",type(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['exact_matched_patt_contextual']=data['exact_matched_patt_contextual'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, l in enumerate(data[\"exact_matched_patt_contextual\"]):\n",
    "#  print(\"list\",i,\"is\",type(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"exact_matched_patt_contextual\"] = data[\"exact_matched_patt_contextual\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_dict = {}\n",
    "for i in data['exact_matched_patt_contextual']:\n",
    "    for j in i:\n",
    "        if j not in tech_dict:\n",
    "            tech_dict[j] = 1\n",
    "        else:\n",
    "            tech_dict[j] += 1\n",
    "\n",
    "# tech_dict\n",
    "\n",
    "tech_list=[]\n",
    "tech_multi_words=[]\n",
    "for i in tech_dict:\n",
    "    if ((' ' in i) == True):\n",
    "        tech_multi_words.append(i)\n",
    "    else:\n",
    "        tech_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_multi_words.append('microsoft power point')\n",
    "tech_multi_words.append('sap pm')\n",
    "tech_multi_words.append('sap erp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 76)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tech_multi_words), len(set(tech_multi_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('pwc' in tech_list), \n",
    "print('ey' in tech_list), \n",
    "print('amazon' in tech_list),\n",
    "print('flipkart' in tech_list),\n",
    "print('microsoft' in tech_list),\n",
    "print('sap' in tech_list),\n",
    "print('ibm' in tech_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_list.append(\"amazon\")\n",
    "tech_list.append(\"flipkart\")\n",
    "tech_list.append(\"microsoft\")\n",
    "tech_list.append(\"sap\")\n",
    "tech_list.append(\"ibm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tech_multi_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tech_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 319)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tech_list) , len(set(tech_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"summaries\"]=data[\"summaries\"].apply(clean_text)\n",
    "data.summaries = data.summaries.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub) # use start += 1 to find overlapping matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lst):\n",
    "    return ' '.join(lst).split()\n",
    "\n",
    "\n",
    "def BIO_conversion(sentence):\n",
    "    \n",
    "    ind_dict = {}\n",
    "    for i in tech_multi_words:\n",
    "        if i in sentence:\n",
    "#             ind_dict[sentence.index(i)]=i #word as val, key as index\n",
    "            all_occ = list(find_all(sentence,i)) # [0, 5, 10, 15]\n",
    "            for ind in all_occ:\n",
    "                ind_dict[ind]=i\n",
    "\n",
    "            \n",
    "    temp_word=\"\"\n",
    "    final_word_sen=[]\n",
    "    final_tag_sen = []\n",
    "    start=-1\n",
    "    end=-1\n",
    "    for i in range(0, len(sentence)):\n",
    "        if(i in range(start,end)):\n",
    "#             print(\"break hua\")\n",
    "            continue\n",
    "#         print(i)\n",
    "        if sentence[i].isspace()==True and len(temp_word)>0:\n",
    "            final_word_sen.append(temp_word)\n",
    "            if temp_word in tech_list:\n",
    "#                 print(temp_word)\n",
    "                final_tag_sen.append(\"B\")\n",
    "            else:\n",
    "                final_tag_sen.append(\"O\")\n",
    "            temp_word=\"\"\n",
    "        else: #not space\n",
    "            try: #do we have multiword at this index\n",
    "                temp_word=ind_dict[i]\n",
    "#                 print(\"****\")\n",
    "                start=i+1\n",
    "                end=i+len(temp_word) #here tag these multiwords and update new i\n",
    "#                 print(i)\n",
    "#                 print(\"****\")\n",
    "#                 print(temp_word)\n",
    "                listofwords = convert([temp_word])\n",
    "                f=True\n",
    "                for word in listofwords:\n",
    "                    final_word_sen.append(word)\n",
    "                    if f:\n",
    "                        final_tag_sen.append(\"B\")\n",
    "                        f=False\n",
    "                    else:\n",
    "                        final_tag_sen.append(\"I\")\n",
    "                temp_word=\"\"\n",
    "            except:\n",
    "                if(sentence[i]!=' '):\n",
    "                    temp_word+=sentence[i]\n",
    "                \n",
    "#     final_tag_sen.append(None) #adding blank\n",
    "#     final_word_sen.append(None)#adding blank\n",
    "#     print(len(sentence.split())-len(final_word_sen))\n",
    "    return pd.DataFrame(list(zip(final_word_sen, final_tag_sen)),columns =['Word', 'Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIO_conversion(data[\"summaries\"][18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=pd.DataFrame()\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "      \n",
    "    temp = BIO_conversion(data[\"summaries\"][i])\n",
    "    length = len(temp)\n",
    "    wordd=\"Sentence :\"+str(i+1) #sentence no.\n",
    "    a=[wordd]*length\n",
    "    temp.insert(0,\"Sentence #\",a)\n",
    "    final_data = final_data.append(temp, ignore_index=True) #appending sentences in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohitbagaria/anaconda3/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9955029f28>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(final_data['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O    320775\n",
       "B      2303\n",
       "I       312\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=final_data\n",
    "df.loc[(df['Tag'] == 'B'), 'Tag'] = 'B-ORG'\n",
    "df.loc[(df['Tag'] == 'I'), 'Tag'] = 'I-ORG'\n",
    "df.to_csv(\"BIO_taggingdata_ALL.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bachelor of science bs degree in telecommunications networks and software program at politehnica university electronics telecommunications and information technology faculty master of science ms degree in management of networks and services program at politehnica university electronics telecommunications and information technology faculty routing and switching skills cisco ccna 640802 certification cisco ccnp route 642902 certification implementing cisco ip routing cisco ccnp switch 642813 certification implementing cisco ip switched networks cisco ccnp tshoot 642832 certification troubleshooting and maintaining cisco ip networks experience in troubleshooting and deploying networks using major l2 l3 protocols and platforms routing protocols rip ospf eigrp isis bgp data link protocols stp rstp mstp first hop redundancy protocols hsrp vrrp glbp mpls fundamentals cisco nexus juniper alcatel sr platforms unix linux basic master of science ms degree in management of networks and services politehnica university electronics telecommunications and information technology faculty bachelor of science bs in telecommunications networks and software politehnica university electronics telecommunications and information technology faculty experience in operation and maintenance planning and deploying large scale networks using high availability routing and switching technologies in the likes of hsrp vrrp ospf bgp mpls ldp experience in working with network automation and deployment tools and scripts using versioned repositories git jinja templates bash python scripts cisco certified network professional routing and switching ccnp routing and switching politehnica university of bucharest graduate in 2011 electronics telecommunications and information technology faculty master in networks and services management at the same faculty diploma obtained in july 2013bachelor degree in telecommunications software and networks routing protocols rip ospf eigrp isis bgp data link protocols stp rstp mstp cdp ppp first hop redundancy protocols hsrp vrrp glbp mpls fundamentals cisco nexus juniper alcatel sr platforms unix linux basic good knowledge of microsoft office suite word excel power point visiospecialtiescisco routing and switchingcisco ccna certifiedcisco ccnp route diplomasdh dwdm basic level mihnea tudor network development engineer internet services at amazon bachelor of science bs degree in telecommunications networks and software program at politehnica university electronics cisco nexus juniper alcatel sr platforms network engineer at workday linkedin members in ireland senior network analyst at continent 8 technologies network specialist at asavie'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"summaries\"][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=final_data2\n",
    "# df.loc[(df['Tag'] == 'B'), 'Tag'] = 'B-ORG'\n",
    "# df.loc[(df['Tag'] == 'I'), 'Tag'] = 'I-ORG'\n",
    "# df.to_csv(\"BIO_taggingdata_B_MUST.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"5k_manual_tag_aman - Iteration_2_corrected sheet_with_30k_as_dictionary.csv\")\n",
    "# data2.head()\n",
    "\n",
    "data2=data2.iloc[0:,0:3]\n",
    "\n",
    "data2.columns=['id','summary','manually tagged']\n",
    "# data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1001, 2), (3633, 3))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape  , data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"tech_sort1k.csv\")\n",
    "data=data.drop(columns=[\"Note\"])\n",
    "data.head(10)\n",
    "data['manually tagged']=data['exact_matched_patt_contextual']\n",
    "\n",
    "data=data.drop(columns=\"exact_matched_patt_contextual\")\n",
    "data['manually tagged'] = [ [] if x is np.NaN else x for x in data['manually tagged'] ]\n",
    "\n",
    "\n",
    "data.rename(columns={'summaries':'summary'}, inplace=True)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# newdf2 = data.append(data2, how=\"outer\",on=['id','summary','manually tagged'])\n",
    "newdf2 = pd.concat([data,data2])\n",
    "newdf2=newdf2.drop_duplicates(subset='id', keep=\"last\")\n",
    "newdf2 = newdf2.reset_index(drop=True)\n",
    "\n",
    "# for i, l in enumerate(newdf2[\"manually tagged\"]):\n",
    "#  print(\"list\",i,\"is\",type(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf2['manually tagged'] = [ [] if x is np.NaN else x for x in newdf2['manually tagged'] ]\n",
    "newdf2['manually tagged']=newdf2['manually tagged'].astype(str)\n",
    "\n",
    "newdf2['manually tagged'] = newdf2['manually tagged'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_dict = {}\n",
    "for i in newdf2['manually tagged']:\n",
    "    for j in i:\n",
    "        if j not in tech_dict:\n",
    "            tech_dict[j] = 1\n",
    "        else:\n",
    "            tech_dict[j] += 1\n",
    "\n",
    "tech_list=[]\n",
    "tech_multi_words=[]\n",
    "for i in tech_dict:\n",
    "    if ((' ' in i) == True):\n",
    "        tech_multi_words.append(i)\n",
    "    else:\n",
    "        tech_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf2[\"summary\"]=newdf2[\"summary\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data=pd.DataFrame()\n",
    "\n",
    "# for i in range(0,len(data)):\n",
    "#     temp = in_techwords(newdf2[\"summary\"][i])\n",
    "#     length = len(temp)\n",
    "#     wordd=\"Sentence :\"+str(i+1) #sentence no.\n",
    "#     a=[wordd]*(length-1)\n",
    "#     a.append(None)\n",
    "#     temp.insert(0,\"Sentence #\",a)\n",
    "#     final_data = final_data.append(temp, ignore_index=True) #appending sentences in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        320775\n",
       "B-ORG      2303\n",
       "I-ORG       312\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[\"Tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_filter = final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data2=pd.DataFrame()\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    temp = BIO_conversion(newdf2[\"summary\"][i])\n",
    "    length = len(temp)\n",
    "    wordd=\"Sentence :\"+str(i+1) #sentence no.\n",
    "    a=[wordd]*(length-1)\n",
    "    a.append(None)\n",
    "    temp.insert(0,\"Sentence #\",a)\n",
    "#     if flag==True:\n",
    "    final_data2 = final_data2.append(temp, ignore_index=True) #appending sentences in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O    370577\n",
       "B      4471\n",
       "I       327\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data2[\"Tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data2.to_csv(\"Manually_tagged_BIO.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda5fa6ef65aabe435e83eb42db00346061"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
